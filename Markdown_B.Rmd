---
title: "Homework 01 Part B"
author: "Verdini - Romeo"
id: " - 1618216"
date: "12/11/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## PART 2

2. Write a program in R to simulate the preferential attachment process, starting with 4 pages linked together as a directed cycle on 4 vertices, adding pages each with one outlink until there are 1 million pages, and using $\gamma$ = 0.5 as the probability a link is to a page chosen uniformly at random and 1 − $\gamma$  = 0.5 as the probability a link is copied from existing links.

```{r include=TRUE, message = FALSE, warning = FALSE}
# libraries used:
library(igraph)
library(mc2d)
library(sdpt3r)
library(glue)
library(ggplot2)
library(gridExtra)
library(gt)
library(knitr)
```

```{r include=FALSE}
set.seed(1765820)
```

In order to generate a graph it has been decided to represent it has list of edges. Our implementation uses two vectors in order to speed up the process even further.

```{r include=TRUE, eval=TRUE}
rep.row<-function(x,n){
  matrix(rep(x,each=n),nrow=n)
} # the function is used to pre-allocate the matrix of all the edges

# the "graph_generator" generates the graph
graph_generator <- function(Num_nodes, gamma=.5){
  
  E_one=1:Num_nodes
  E_two=rep(0,Num_nodes) 
  # we found out that using 2 vectors instead of a matrix with 2 columns gives a      minor 
  # boost in the computation speed.
  E_two[1] <-2 
  E_two[2] <-3
  E_two[3] <-4
  E_two[4] <-1
  
  
  for (new_vertex in 5:Num_nodes){
    if (rbern(n = 1, prob=gamma) ==1) {
      get_vertex = sample(E_one[1:(new_vertex-1)], 1) 
      # choosing existing node uniformly    
      E_two[new_vertex] <-get_vertex
    }
    else {
      get_vertex = sample(E_two[1:(new_vertex-1)], 1) 
      # choosing existing node with a weighted distribution (based on the in-degree)
      E_two[new_vertex] <-get_vertex
    }
  }
  results = list()
  results$one = E_one
  results$two = E_two
  return(results)
} 
```

The "cumulative_empirical" function computes the empirical degree distribution and cumulative degree distribution. It has been decided to work with a matrix becouse in this way we can return the 3 results in 1 object; more specifically: 
- in degree value,
- empirical degree distribution,
- cumulative degree distribution.

```{r include=TRUE, eval=TRUE}
cumulative_empirical = function(E){
  
  E_one = E$one # staring node vector 
  E_two = E$two # ending node vector
  Num_nodes = length(E$one)
  
  # we iterate through all the numbers in the E_two (which is the vector containing   # the end-nodes of
  # the edges) and add one to the corresponding element in vector in_nodes
  # (which is the vector containing the in-degree of the nodes).  
  # For example: 
  # if E_two = [2 3 4 1 2 2 5 2 3 5 5 5], we first add 1 to the second element of 
  # the in_nodes vector, then to the third, then to the fourth, then first, second,
  # second, fifth, etc...
  # At the end, the in_nodes vector will be: [1 4 2 1 4 0 0 0 0 0 0 0] meaning that
  # the in-degree of node 1 is 1, the in-degree of node 2 is 4, etc...

  in_nodes=rep(0,Num_nodes) # pre-allocate a vector with 0
  for (i in 1:Num_nodes){
    in_nodes[E_two[i]]=in_nodes[E_two[i]]+1
  }
  
  sorted_in_nodes=sort(in_nodes,decreasing=TRUE)
  indexes=unique(sorted_in_nodes) # it's the number of nodes where the cumulative     changes
  
  A=rep.row(c(0,0,0),length(indexes)) 
  # pre-allocate the matrix. In the first column there are the values representing
  # the node where there is at least awhere there is 
  # 
  
  A[,1]=indexes
  
  counter_cumulative = 0
  counter_empirical = 0
  j=1
  for (i in 1:length(indexes)){
    in_degree_value=A[i,1]
    counter_empirical = 0 # here we reset the counter every time we change in-degree node
    # because it is used to compute the empirical distribution.
    while(j<=Num_nodes && sorted_in_nodes[j]==in_degree_value){
      
      # we calculate the empirical distribution with a single
      # loop trough the vector: we use the fact that vector is already sorted,
      # so it is not necessary to look all the elements once we find the 
      # first one which is different from the current in-degree value.
      counter_empirical = counter_empirical +1
      j=j+1
    } 
    
    counter_cumulative = counter_cumulative + counter_empirical
    A[i,3]=counter_cumulative
    A[i,2]=counter_empirical
  }
  
  empirical = rev(A[,2])[-1]
  cumulative = rev(A[,3])[-1]
  in_degree_value = rev(A[,1])[-1]
  return(A)

}
```

The code below generates 5 graphs with 1 million edges, and store each of them inside a network_{i}.csv file.
```{r include=TRUE, eval=FALSE}
for (i in 1:5){
  A = cumulative_empirical(graph_generator(1000000))
  write.csv(A, glue("networks_final/network_{i}.csv"), row.names = F)
  print(glue('graph {i} stored'))
}

```


2. Does the degree distribution appear to follow a power law or a Poisson? Explain and comment by showing suitable visual and numerical evidence that supports your reasoning.



```{r include=TRUE, eval=TRUE}

D1 = as.data.frame(read.csv("distributions_matrices/cum_dev_matrix_1.csv"))
D2 = as.data.frame(read.csv("distributions_matrices/cum_dev_matrix_2.csv"))
D3 = as.data.frame(read.csv("distributions_matrices/cum_dev_matrix_3.csv"))
D4 = as.data.frame(read.csv("distributions_matrices/cum_dev_matrix_4.csv"))
D5 = as.data.frame(read.csv("distributions_matrices/cum_dev_matrix_5.csv"))

```




```{r include=TRUE, eval=TRUE}

par(mfrow=c(1,2))
# Create a first line
plot(x = log(D1$V1), y=log(D1$V3), type = "l", col = "red", lty=1, lwd=2,
     xlab = "K in log scale", ylab = "cumulative in log scale",
     main="Cumulative in log-log scale")
# Add a second line
lines(x = log(D2$V1), y=log(D2$V3),  col = "blue", type = "l", lwd=2, lty = 2)#, log = 'xy')
# Add a third line
lines(x = log(D3$V1), y=log(D3$V3),  col = "green", type = "l", lwd=2, lty = 3)#, log = 'xy')
# Add a fourth line
lines(x = log(D4$V1), y=log(D4$V3),  col = "orange", type = "l", lwd=2, lty = 4)#, log = 'xy')
# Add a fifth line
lines(x = log(D5$V1), y=log(D5$V3),  col = "purple", type = "l", lwd=2, lty = 5)#, log = 'xy')
# Add a legend to the plot
legend("topright", legend=c("Graph 1", "Graph 2", "Graph 3", "Graph 4", "Graph 5"),
      col=c("red", "blue", "green", "orange", "purple"), lty = 1:5, cex=0.5, bty='o',
      bg='lightgrey', title = "Networks:")


# Create a first line
plot(x = log(D1$V1), y=log(D1$V2), type = "l", col = "red", lty=1, lwd=2,
     xlab = "K in log scale", ylab = "empirical in log scale",
     main="Empirical in log-log scale")#, log = 'xy')
# Add a second line
lines(x = log(D2$V1), y=log(D2$V2),  col = "blue", type = "l", lwd=2, lty = 2)#, log = 'xy')
# Add a third line
lines(x = log(D3$V1), y=log(D3$V2),  col = "green", type = "l", lwd=2, lty = 3)#, log = 'xy')
# Add a fourth line
lines(x = log(D4$V1), y=log(D4$V2),  col = "orange", type = "l", lwd=2, lty = 4)#, log = 'xy')
# Add a fifth line
lines(x = log(D5$V1), y=log(D5$V2),  col = "purple", type = "l", lwd=2, lty = 5)#, log = 'xy')
# Add a legend to the plot
legend("topright", legend=c("Graph 1", "Graph 2", "Graph 3", "Graph 4", "Graph 5"),
       col=c("red", "blue", "green", "orange", "purple"), lty = 1:5, cex=0.5, bty='o',
       bg='lightgrey', title = "Networks:")
```

As a preliminary analysis we simply try to see how close our distributions are to a power law visually. For that reason we simply plotted the regression lines on the distributions and see how close they are to a straight line.  


```{r include=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
p1 <- ggplot(D1, aes(x=log(V1), y=log(V3))) +
  geom_point(size=1, color="#DE8CF0")+
  geom_smooth(method=lm, se=FALSE, linetype="dashed", color="#BED905")+ 
  ggtitle("Cumulative & fitting line for Graph 1") +
  xlab("K in log scale") + ylab("Cumulative in log scale") +
  theme(
    plot.title = element_text(color="#DE8CF0", size=8, face="bold.italic"),
    axis.title.x = element_text(color="black", size=6, face="bold"),
    axis.title.y = element_text(color="black", size=6, face="bold")
  )

p2 <- ggplot(D2, aes(x=log(V1), y=log(V3))) +
  geom_point(size=1, color="#525B56")+
  geom_smooth(method=lm, se=FALSE, linetype="dashed", color="#BE9063")+ 
  ggtitle("Cumulative & fitting line for Graph 2") +
  xlab("K in log scale") + ylab("Cumulative in log scale") +
  theme(
    plot.title = element_text(color="#525B56", size=8, face="bold.italic"),
    axis.title.x = element_text(color="black", size=6, face="bold"),
    axis.title.y = element_text(color="black", size=6, face="bold")
  )

p3 <- ggplot(D3, aes(x=log(V1), y=log(V3))) +
  geom_point(size=1, color="#00743F")+
  geom_smooth(method=lm, se=FALSE, linetype="dashed", color="#F2A104")+ 
  ggtitle("Cumulative & fitting line for Graph 3") +
  xlab("K in log scale") + ylab("Cumulative in log scale") +
  theme(
    plot.title = element_text(color="#00743F", size=8, face="bold.italic"),
    axis.title.x = element_text(color="black", size=6, face="bold"),
    axis.title.y = element_text(color="black", size=6, face="bold")
  )

p4 <- ggplot(D4, aes(x=log(V1), y=log(V3))) +
  geom_point(size=1, color="#36688D")+
  geom_smooth(method=lm, se=FALSE, linetype="dashed", color="#F49F05")+ 
  ggtitle("Cumulative & fitting line for Graph 4") +
  xlab("K in log scale") + ylab("Cumulative in log scale") +
  theme(
    plot.title = element_text(color="#36688D", size=8, face="bold.italic"),
    axis.title.x = element_text(color="black", size=6, face="bold"),
    axis.title.y = element_text(color="black", size=6, face="bold")
  )

p5 <- ggplot(D5, aes(x=log(V1), y=log(V3))) +
  geom_point(size=1, color="#0294A5")+
  geom_smooth(method=lm, se=FALSE, linetype="dashed", color="#C1403D")+ 
  ggtitle("Cumulative & fitting line for Graph 5") +
  xlab("K in log scale") + ylab("Cumulative in log scale") +
  theme(
    plot.title = element_text(color="#0294A5", size=8, face="bold.italic"),
    axis.title.x = element_text(color="black", size=6, face="bold"),
    axis.title.y = element_text(color="black", size=6, face="bold")
  )

graph_names <- c("graph 1","graph 2","graph 3","graph 4","graph 5")
lm_function <- rep(0,5)
df_list = list(D1, D2, D3, D4, D5)
counter = 1

for (df in df_list){
  c_lm <- lm(V3~V1, data = log(df[-nrow(df),])) # removing last row because it has 0
  strg <- paste("y = ",toString(round(c_lm$coefficients[[1]],2)),
                paste(toString(round(c_lm$coefficients[[2]],2)), 'x', sep=""),
                sep=" ")
  lm_function[counter] = strg
  counter = counter +1
}

grid.arrange(p1, p2, p3, p4, p5, nrow=2, ncol=3)

```

WARNING: Be careful, the following coefficients and the intercepts are shown only to give an idea of the slope of the regression lines, and are not to be taken as a serious statistical test. 

```{r}
# Table
kable(data.frame(cbind(graph_names, lm_function)), caption= "Fitted regression lines")
```


By digging into the igraph library we found a function called fit_power_law(), that fits the empirical data with a power law distribution and then computes the goodness of the fit with the Kolmogorov–Smirnov test. This should return a more robust statistical result. The function returns the KS_stat and the p-value. The KS_stat tells us how well the power law distribution fits our data, i.e to a lower value better corresponds a better fit. However, this measure alone is not enough to tell how likely it is that data is drawn from a power law. For that reason the function also returns the p-value. This is a two-sided test, with a confidence interval $\alpha$ that has to be chosen when running the test, usually the used values are 1%, 3%, 5%. In this case it is irrelevant which value of $\alpha$ one chooses because the returned p-value is very close to one (when rounded to two decimals, as in the table below, it is exactly 1). If the p-value is above the critical value (p-value > $\alpha$) then, we accept the $H_{0}$ otherwise we reject $H_{0}$.
Below is reported a summary table with the KS_statistics and the p-value:

```{r echo = FALSE, results = 'asis', include=TRUE}

ks_stat <- rep(0, 5)
p_value <- rep(0, 5)
m = 1
for (df in df_list){
  
  results <- fit_power_law(df$V2, xmin=1)
  ks_stat[m] <- round(results$KS.stat,2)
  p_value[m] <- round(results$KS.p,2)
  m = m+1
}

ks_df <- data.frame(cbind(graph_names, ks_stat, p_value))
kable(ks_df, caption= "Kolmogorov-Smirnov Goodness of Fit Test")
```


As a comparison, this is the result of a KS test on a graph generated with $\gamma$ = 1 

```{r}

#graph_gamma = graph_generator(1000, 1)


```








